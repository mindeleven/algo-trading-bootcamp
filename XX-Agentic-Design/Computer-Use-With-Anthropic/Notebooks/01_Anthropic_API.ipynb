{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7c5be6-8be1-4dd7-b741-9c6a5d025235",
   "metadata": {},
   "source": [
    "## The Anthropic API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181e157-a3e1-4fa0-906d-b9b2bd7e54a9",
   "metadata": {},
   "source": [
    "*(Coding along with deeplearning.ai's online course [Building toward Computer Use with Anthropic - Learn how an AI Assistant is built to use and accomplish tasks on computers](https://learn.deeplearning.ai/courses/building-toward-computer-use-with-anthropic/lesson/1/introduction) taught by Colt Steele)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f9082-7393-4265-b8b9-f40b079d6627",
   "metadata": {},
   "source": [
    "### Working with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6a639f-50d4-4bb3-be6f-c20527e57729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.deeplearning.ai/courses/building-toward-computer-use-with-anthropic/lesson/3/working-with-the-api\n",
    "rom anthropic import Anthropic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e804c22-2002-44c5-bf51-3e53218282ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "anthropic_api_key = pd.read_csv(\"~/tmp/anthropic/anthropic-key-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe3d4ea-253d-4f0c-9268-80ca0ee97876",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999d7cd1-8a8f-462c-9cc0-5ebf6b76edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"claude-3-5-sonnet-20241022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5c22f0-19f8-4dec-ab6e-535877c1a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a haiku about Anthropic:\n",
      "\n",
      "Seeking truth and light\n",
      "Building AI with purpose\n",
      "For humanity\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about Anthropic\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb414588-581e-45f6-999d-c7eb499aedff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_016xTaDooynjLQyqTTG4jHdP', content=[TextBlock(citations=None, text=\"Here's a haiku about Anthropic:\\n\\nSeeking truth and light\\nBuilding AI with purpose\\nFor humanity\", type='text')], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=15, output_tokens=27))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# having a look at the response object\n",
    "response # response back is a role of \"assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed20e5-93d0-428b-be6f-2210a48ab92b",
   "metadata": {},
   "source": [
    "### The Messages Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40de700e-b931-4b09-b129-38327eb7a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Estoy muy bien, gracias! ¿Y tú? ¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Only speak to me in Spanish\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hola!\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd059129-07bb-4b16-8f4b-f5732d0650d3",
   "metadata": {},
   "source": [
    "#### __Simple Chatbot: Building a Conversational Assistant__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44fb0f8-e4f8-48de-b43f-4a4409052476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Chatbot (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello! I'm Jürgen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello Jürgen! Nice to meet you. How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What's my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your name is Jürgen.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Help my to learn a bit more about how LLMs work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'll give you a basic overview of how Large Language Models (LLMs) like myself work:\n",
      "\n",
      "1. Training Process:\n",
      "- LLMs are trained on massive amounts of text data from the internet, books, and other sources\n",
      "- They learn patterns in language by predicting what words might come next in a sequence\n",
      "- This training creates a complex network of statistical relationships between words and concepts\n",
      "\n",
      "2. Key Components:\n",
      "- Transformer architecture: The underlying structure that allows processing of text\n",
      "- Attention mechanisms: Help the model focus on relevant parts of the input\n",
      "- Parameters: Billions of adjustable values that store the learned patterns\n",
      "\n",
      "3. How they work:\n",
      "- When you input text, the model processes it token by token (pieces of words)\n",
      "- It uses its trained parameters to predict the most likely appropriate response\n",
      "- The response is generated word by word based on the context\n",
      "\n",
      "4. Limitations:\n",
      "- No true understanding or consciousness\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Expand on the fourth item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Here's a detailed expansion on LLMs' limitations:\n",
      "\n",
      "1. No True Understanding:\n",
      "- LLMs don't actually \"understand\" text like humans do\n",
      "- They operate on statistical patterns rather than genuine comprehension\n",
      "- They can't form real mental models or truly reason about concepts\n",
      "\n",
      "2. No Consciousness or Emotions:\n",
      "- Despite seeming conversational, LLMs don't have feelings or self-awareness\n",
      "- They can discuss emotions but don't experience them\n",
      "- Their responses about feelings are based on patterns in training data\n",
      "\n",
      "3. Factual Limitations:\n",
      "- Can generate plausible-sounding but incorrect information (\"hallucinations\")\n",
      "- Knowledge is limited to training data cutoff date\n",
      "- Can't learn from conversations or update their knowledge\n",
      "\n",
      "4. Contextual Limitations:\n",
      "- Limited context window (can only consider a finite amount of previous text)\n",
      "- May lose track of longer conversations\n",
      "- Can be incons\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Chatbot (type 'quit' to exit)\")\n",
    "# store conversation history\n",
    "\n",
    "# starting with an empty list of messages\n",
    "messages = []\n",
    "\n",
    "# we'll loop until the user puts in the word 'quit'\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "    # Check for quit command\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # Add user message to history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    try:\n",
    "        # Get response from Claude\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=200,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract and print Claude's response\n",
    "        asst_message = response.content[0].text\n",
    "        print(\"Assistant:\", asst_message)\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": asst_message})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a719b9-2428-406a-8c72-d18453f9cdc2",
   "metadata": {},
   "source": [
    "#### __Prefilling the Assistant Response: Putting Words into the Model's Mouth__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00672062-7584-4f60-a905-c3e29c9077e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " animals!. They get around when they feel like it. Stealthy like a ghost jumping high above! A friendly creature. They love to roam around and sometimes you can see them doing their own things.\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about cats\"},\n",
    "        # telling the model which word it should start\n",
    "        # skips also any kind of intro text\n",
    "        {\"role\": \"assistant\", \"content\": \"Narcisstic\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a5728-a882-4c55-a1d9-c3b2f946e30b",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7b422-1917-457f-9304-dc3c0e893fd1",
   "metadata": {},
   "source": [
    "#### __Max Tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d08d39c-f3d1-4fe1-bb77-a2142591d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an essay on Large Language Models (LLMs):\n",
      "\n",
      "Large Language Models: Reshaping the Future of Artificial Intelligence\n",
      "\n",
      "Large Language Models (LLMs) have emerged as one of the most significant developments in artificial intelligence over the past few years, fundamentally transforming how we interact with machines and process information. These sophisticated AI systems, trained on vast amounts of text data, have demonstrated remarkable capabilities in understanding and generating human-like text, leading to unprecedented advances in natural language\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    # the maximum number of tokens that claude should generate in it's response\n",
    "    # it's setting an upper bound\n",
    "    max_tokens=100,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me an essay on LLMs\"},\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931434f2-2a22-4421-b624-fc177a8e2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01DSS6WesRnGCe7b98a43g6H', content=[TextBlock(citations=None, text=\"Here's an essay on Large Language Models (LLMs):\\n\\nLarge Language Models: Reshaping the Future of Artificial Intelligence\\n\\nLarge Language Models (LLMs) have emerged as one of the most significant developments in artificial intelligence over the past few years, fundamentally transforming how we interact with machines and process information. These sophisticated AI systems, trained on vast amounts of text data, have demonstrated remarkable capabilities in understanding and generating human-like text, leading to unprecedented advances in natural language\", type='text')], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason='max_tokens', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=15, output_tokens=100))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b1cec-a782-492e-a84e-6e0432f66dd4",
   "metadata": {},
   "source": [
    "#### __Stop Sequences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "152d21a1-3a83-4d62-bce6-99b7591d087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Python Programming\n",
      "   - Data structures and algorithms\n",
      "   - Object-oriented programming\n",
      "   - Code optimization\n",
      "   - Package management\n",
      "\n",
      "2. Machine Learning Fundamentals\n",
      "   - Linear algebra\n",
      "   - Probability and statistics\n",
      "   - Gradient descent\n",
      "   - Neural network basics\n",
      "   - Backpropagation\n",
      "\n",
      "3. Deep Learning\n",
      "   - Deep neural networks\n",
      "   - Activation functions\n",
      "   - Loss functions\n",
      "   - Optimization algorithms\n",
      "   - Regularization techniques\n",
      "   - PyTorch/TensorFlow frameworks\n",
      "\n",
      "4. Natural Language Processing\n",
      "   - Text preprocessing\n",
      "   - Tokenization\n",
      "   - Word embeddings\n",
      "   - Language modeling\n",
      "   - NLTK/spaCy libraries\n",
      "\n",
      "5. Transformer Architecture\n",
      "   - Attention mechanisms\n",
      "   - Self-attention\n",
      "   - Multi-head attention\n",
      "   - Positional encoding\n",
      "   - Encoder-decoder architecture\n",
      "\n",
      "6. LLM-specific Concepts\n",
      "   - Pretraining methods\n",
      "   - Fine-tuning techniques\n",
      "   - Prompt engineering\n",
      "   - Context windows\n",
      "   - Token optimization\n",
      "   - Few-shot learning\n",
      "\n",
      "7. Model Evaluation\n",
      "   - Evaluation metrics\n",
      "   - Benchmarking\n",
      "   - Model validation\n",
      "   - Testing methodologies\n",
      "\n",
      "8. MLOps & Deployment\n",
      "   - Model serving\n",
      "   - Version control\n",
      "   - Experiment tracking\n",
      "   - Resource management\n",
      "   - Containerization (Docker)\n",
      "\n",
      "9. Ethics & Safety\n",
      "   - AI safety principles\n",
      "   - Bias detection/mitigation\n",
      "   - Responsible AI development\n",
      "   - Privacy considerations\n",
      "\n",
      "10. Research Skills\n",
      "    - Paper reading\n",
      "    - Academic writing\n",
      "    - Experiment design\n",
      "    - Literature review\n",
      "    - Reproducible research\n",
      "\n",
      "11. Hardware & Infrastructure\n",
      "    - GPU computing\n",
      "    - Distributed training\n",
      "    - Cloud platforms (AWS, GCP)\n",
      "    - Memory optimization\n",
      "    - Parallel processing\n",
      "\n",
      "12. Development Tools\n",
      "    - Git\n",
      "    - Jupyter notebooks\n",
      "    - IDE proficiency\n",
      "    - Debugging tools\n",
      "    - Performance monitoring\n",
      "\n",
      "This list progresses from foundational knowledge to more specialized LLM-specific topics. It's recommended to build a strong foundation in the earlier topics before moving to more advanced concepts.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a numbered, ordered list of technical topics \n",
    "I should learn if I want to work on LLMs\n",
    "\"\"\"\n",
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=500,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18146c-ff97-474f-bcfa-e5e25d5307b7",
   "metadata": {},
   "source": [
    "In the follwong code example we set stop_sequences to [\"4.\"]. This will stop the output as soon as \"4.\" is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e596a90f-0626-4407-b81c-e9cc196478b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a suggested learning path for working with LLMs:\n",
      "\n",
      "1. Programming Fundamentals\n",
      "   - Python programming\n",
      "   - Object-oriented programming\n",
      "   - Version control (Git)\n",
      "   - Command line basics\n",
      "\n",
      "2. Mathematics & Statistics\n",
      "   - Linear algebra\n",
      "   - Probability and statistics\n",
      "   - Calculus\n",
      "   - Information theory\n",
      "\n",
      "3. Machine Learning Basics\n",
      "   - Supervised/unsupervised learning\n",
      "   - \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a numbered, ordered list of technical topics \n",
    "I should learn if I want to work on LLMs\n",
    "\"\"\"\n",
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=500,\n",
    "    # a list of strings that tells the model when to stop\n",
    "    # it stops before\n",
    "    stop_sequences=[\"4.\", \"Neural networks\"],\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd5bb9ba-1736-4b89-a488-ce7cfa66630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01Cq9pBWHwXrdimRaCbKqZ8t', content=[TextBlock(citations=None, text=\"Here's a suggested learning path for working with LLMs:\\n\\n1. Programming Fundamentals\\n   - Python programming\\n   - Object-oriented programming\\n   - Version control (Git)\\n   - Command line basics\\n\\n2. Mathematics & Statistics\\n   - Linear algebra\\n   - Probability and statistics\\n   - Calculus\\n   - Information theory\\n\\n3. Machine Learning Basics\\n   - Supervised/unsupervised learning\\n   - \", type='text')], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason='stop_sequence', stop_sequence='Neural networks', type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=32, output_tokens=98))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n",
    "# response tells us, why the model stopped: stop_reason='stop_sequence', stop_sequence='4.'\n",
    "# adding \"Neural networks\": stop_reason='stop_sequence', stop_sequence='Neural networks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f56cad-32c6-40c3-8320-a9b39a1aa098",
   "metadata": {},
   "source": [
    "#### __Temperature__\n",
    "\n",
    "Temperature could be seen as the randomness or creativity of the generated responses. \n",
    "\n",
    "Temperature ranges from zero to one where a higher value results into more unpredictable and diverse responses.\n",
    "\n",
    "Example experiment: the model got asked to pick an animal. With a temperature of 0 every single response out of 100 was the word \"Giraffe\". With temperature set to 1 we get more differnt animals in the response (more variation). A temperature of 0 is more deterministic.\n",
    "\n",
    "<img src=\"../assets/images/temperature.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1bf61c8-a5f7-428e-991b-ccaad0c96cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_temperature():\n",
    "    temperatures = [0, 1]\n",
    "    for temperature in temperatures:\n",
    "        print(\"================\")\n",
    "        print(f\"Prompting Claude three times with temperature of {temperature}\")\n",
    "        print(\"================\")\n",
    "        for i in range(5):\n",
    "            response = client.messages.create(\n",
    "                model=MODEL_NAME,\n",
    "                max_tokens=100,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Prompt {i+1}: Come up with a name for an alien planet. Respond with a single word.\"}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            print(f\"Response {i+1}: {response.content[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2a6dd75-1dbe-4003-9c8d-c006dfa80eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Prompting Claude three times with temperature of 0\n",
      "================\n",
      "Response 1: Kestrax\n",
      "Response 2: Zephyrix\n",
      "Response 3: Kestrax\n",
      "Response 4: Zephyrix\n",
      "Response 5: Zephyrix\n",
      "================\n",
      "Prompting Claude three times with temperature of 1\n",
      "================\n",
      "Response 1: Zeldraxis\n",
      "Response 2: Xylaxis\n",
      "Response 3: Zeyphrix\n",
      "Response 4: Xylora\n",
      "Response 5: Zephyrix\n"
     ]
    }
   ],
   "source": [
    "demonstrate_temperature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e7ee1-6a8e-4e82-8f2e-62dc6b6ea06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
