{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff3ca54-e09a-4364-ae6e-6524067df7ee",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bca428-8573-493c-bddf-adc417cb533a",
   "metadata": {},
   "source": [
    "*(Coding along with the [Anthropic API fundamentals](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals) of Anthropic's courses GitHub repo)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074878c-869f-4462-a6ee-8993da0a9518",
   "metadata": {},
   "source": [
    "The goals of the lesson are understanding the role of the `max_tokens` parameter, using the `temperature` parameter to control model responses and explaining the purpose of `stop_sequence`.\n",
    "\n",
    "The following three parameters are __required__ with every call we send to the Claude model:\n",
    "- model\n",
    "- max_tokens\n",
    "- messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3d954-3272-4944-b97f-a1da20fe7b41",
   "metadata": {},
   "source": [
    "### Basis setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10a69e02-ebd0-4595-a810-793238640f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/04_parameters.ipynb\n",
    "from anthropic import Anthropic\n",
    "import pandas as pd\n",
    "\n",
    "anthropic_api_key = pd.read_csv(\"~/tmp/anthropic/anthropic-key-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")\n",
    "\n",
    "# instantiating the client\n",
    "client = Anthropic(api_key=anthropic_api_key)\n",
    "MODEL_NAME=\"claude-3-5-sonnet-20241022\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3bd17-9085-46de-9f8e-1fe57005af48",
   "metadata": {},
   "source": [
    "### Max tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb284e-07c8-46d5-95a4-38236ca9b66e",
   "metadata": {},
   "source": [
    "Example of a request to Claude:\n",
    "```\n",
    "our_first_message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cd414-3aca-469d-83db-e09378c847ae",
   "metadata": {},
   "source": [
    "#### __Tokens__\n",
    "\n",
    "`max_tokens` controls the maximum number of tokens that Claude should generate in its response.\n",
    "\n",
    "Most LLMs work with a series of word-fragments called tokens which are the small building blocks of a text sequence that Claude processes, understands, and uses to generate texts with. A promzt that we provide to Claude is first turned into tokens and then passed to the model. To give us a response, the model generates its output one token at a time.\n",
    "\n",
    "For Claude a token approximately represents 3.5 English characters (the exact number can vary depending on the language used).\n",
    "\n",
    "#### __Working with `max_tokens`__\n",
    "\n",
    "The `max_tokens` parameter allows to set an upper limit on how many tokens Claude generates in it's output. If we ask Claude to write us a poem and set `max_tokens` to 10, Claude will stop generating tokens immediately as soon as it hits 10 tokens.\n",
    "\n",
    "Increasing `max_tokens` does not ensure that Claude actually generates a specific number of tokens.\n",
    "\n",
    "*(Source: https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/04_parameters.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c50fbd9-b31f-46c2-84d0-50f804d8fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a poem for you:\n",
      "\n",
      "Whis\n"
     ]
    }
   ],
   "source": [
    "# let's try it with an example\n",
    "truncated_response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=10, # limiting the tokens for the answer\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(truncated_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28517360-dc37-410a-af04-212d9d90e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01CEPWj157H6WSnUc6wa8vnB', content=[TextBlock(citations=None, text='Here is a poem for you:\\n\\nWhis', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='max_tokens', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=11, output_tokens=10))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0393f8d-dcfa-4f30-b910-4daa278fef73",
   "metadata": {},
   "source": [
    "If we want to check why the model stopped genereatig output, we can have a look at the `stop_reason` property on the response Message object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9eedd18a-2ac8-4dda-a781-230a5304e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_tokens'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_response.stop_reason # stop_reason='max_tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ad49f-8e34-43a7-a4f2-7a258b9855be",
   "metadata": {},
   "source": [
    "We can see that the model has stopped generating output because it hit the `max_tokens` limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a54c6e26-e635-42e3-a3e2-0a87731f85c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a poem for you:\n",
      "\n",
      "Whispers of the Wind\n",
      "\n",
      "The breeze, a gentle caress,\n",
      "Dances across the open field.\n",
      "Caressing the flowers, the trees,\n",
      "Nature's symphony it wields.\n",
      "\n",
      "Soft murmurs, a soothing lullaby,\n",
      "Carried on the zephyr's breath.\n",
      "Rustling leaves, a tranquil reply,\n",
      "As the wind doth gently tread.\n",
      "\n",
      "Amidst the verdant landscape's grace,\n",
      "The breeze weaves its wondrous spell.\n",
      "A moment of serene embrace,\n",
      "Where the wind's sweet secrets dwell.\n"
     ]
    }
   ],
   "source": [
    "# setting max_tokens to 500 to get an entire poem\n",
    "longer_poem_response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(longer_poem_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d92b4aa-9224-409a-b470-cd28cdf62794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01X4fgwg6wscRPZjFTkuYxVg', content=[TextBlock(citations=None, text=\"Here is a poem for you:\\n\\nWhispers of the Wind\\n\\nThe breeze, a gentle caress,\\nDances across the open field.\\nCaressing the flowers, the trees,\\nNature's symphony it wields.\\n\\nSoft murmurs, a soothing lullaby,\\nCarried on the zephyr's breath.\\nRustling leaves, a tranquil reply,\\nAs the wind doth gently tread.\\n\\nAmidst the verdant landscape's grace,\\nThe breeze weaves its wondrous spell.\\nA moment of serene embrace,\\nWhere the wind's sweet secrets dwell.\", type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=11, output_tokens=145))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_poem_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e424c8e5-1ac3-480e-8e4d-2a87065e3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_turn'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_poem_response.stop_reason # \"end_turn\" tells us that the model naturally finished generating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b57555-c752-4155-9358-6d63fa7c72b2",
   "metadata": {},
   "source": [
    "#### __Reasons to alter max tokens:__\n",
    "\n",
    "- __API limits__: The number of tokens in your input text and the generated response count towards the API usage limits. Each API request has a maximum limit on the number of tokens it can process. Being aware of tokens helps you stay within the API limits and manage your usage efficiently.\n",
    "-__Performance__: The number of tokens Claude generates directly impacts the processing time and memory usage of the API. Longer input texts and higher `max_tokens` values require more computational resources. Understanding tokens helps you optimize your API requests for better performance.\n",
    "-__Response quality__: Setting an appropriate `max_tokens` value ensures that the generated response is of sufficient length and contains the necessary information. If the `max_tokens` value is too low, the response may be truncated or incomplete. Experimenting with different max_tokens values can help you find the optimal balance for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef189745-1c74-4ba8-bcc7-4f56721b8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that asks Claude to generate a very long dialogue between two characters three different times, \n",
    "# each with a different value for max_tokens\n",
    "# it then prints out how many tokens were actually generated and how long the generation took\n",
    "import time\n",
    "def compare_num_tokens_speed():\n",
    "    token_counts = [100,1000,4096]\n",
    "    task = \"\"\"\n",
    "        Create a long, detailed dialogue that is at least 5000 words long between two characters discussing the impact of social media on mental health. \n",
    "        The characters should have differing opinions and engage in a respectful thorough debate.\n",
    "    \"\"\"\n",
    "\n",
    "    for num_tokens in token_counts:\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = client.messages.create(\n",
    "            # model=\"claude-3-haiku-20240307\", # Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=num_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": task}]\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        print(f\"Number of tokens generated: {response.usage.output_tokens}\")\n",
    "        print(f\"Execution Time: {execution_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af612be5-cdf7-455a-8d12-876314663af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens generated: 100\n",
      "Execution Time: 1.91 seconds\n",
      "\n",
      "Number of tokens generated: 1000\n",
      "Execution Time: 17.31 seconds\n",
      "\n",
      "Number of tokens generated: 2809\n",
      "Execution Time: 45.57 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_num_tokens_speed() # the more tokens Claude generates, the longer it takes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bb744-7e8f-4a47-b776-968ff44eb956",
   "metadata": {},
   "source": [
    "### Stop sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf1e6b-a8dd-4afa-b021-4726279f5cf6",
   "metadata": {},
   "source": [
    "`stop_sequence` allow us to provide the Claude model with a set of strings that, when encountered in the generated response, cause the generation to stop. \n",
    "\n",
    "Example of a request that does not include a stop_sequence:\n",
    "\n",
    "```\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n",
    ")\n",
    "print(response.content[0].text)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c40f60f-e255-4260-848e-6b596791a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a JSON object representing a person with a name, email, and phone number:\n",
      "\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"email\": \"john.doe@example.com\",\n",
      "  \"phoneNumber\": \"555-1234\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# let's run it\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5453294-82d8-4dee-a139-e94b513dbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a JSON object representing a person with a name, email, and phone number:\n",
      "\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"email\": \"johndoe@example.com\",\n",
      "  \"phoneNumber\": \"555-1234\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now with the stop_sequences parameter:\n",
    "# we want Claude to stop generating as soon as it generated the closing \"}\" of the JSON object \n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n",
    "    stop_sequences=[\"}\"]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82a61427-2eb7-474f-92e6-6518af876fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop_sequence'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.stop_reason # why did Claude stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4458e098-09a5-476f-9c24-db3c8e58c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'}'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.stop_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cba6fd83-7f89-4ef3-9289-d2f3fb6e5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing multiple stop sequences\n",
    "# the model will stop generating as soon as it encounters any of the stop sequences\n",
    "# the resulting stop_sequence property on the response Message will tell us which exact stop_sequence was encountered\n",
    "def generate_random_letters_3_times():\n",
    "    for i in range(3):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"generate a poem\"}],\n",
    "            stop_sequences=[\"b\", \"c\"]\n",
    "        )\n",
    "        print(response.content[0].text)\n",
    "        print(f\">>> Response {i+1} stopped because {response.stop_reason}.  The stop sequence was {response.stop_sequence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f71ff2d-b815-4b11-8a83-872013deb32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is a poem for you:\n",
      "\n",
      "Whispers of the Winds\n",
      "\n",
      "Carried on the \n",
      ">>> Response 1 stopped because stop_sequence.  The stop sequence was b\n",
      "\n",
      "Here is a poem I generated:\n",
      "\n",
      "Whispers on the wind,\n",
      "E\n",
      ">>> Response 2 stopped because stop_sequence.  The stop sequence was c\n",
      "\n",
      "Here is a poem I generated:\n",
      "\n",
      "The Quiet Whisper\n",
      "\n",
      "In the stillness of the night,\n",
      "A gentle \n",
      ">>> Response 3 stopped because stop_sequence.  The stop sequence was b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_random_letters_3_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c8f61-ddf3-406d-8e8c-486143335131",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f2c6c-8b48-4921-9496-f704cd87ecb0",
   "metadata": {},
   "source": [
    "- The temperature parameter is used to control the \"randomness\" and \"creativity\" of the generated responses.\n",
    "- It ranges from 0 to 1, with higher values resulting in more diverse and unpredictable responses with variations in phrasing.\n",
    "- Lower temperatures can result in more deterministic outputs that stick to the most probable phrasing and answers.\n",
    "- Temperature has a default value of 1.\n",
    "- __Use temperature closer to 0.0 for analytical tasks, and closer to 1.0 for creative and generative tasks.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283cc32-91ed-4e42-900e-cb5afdcb9e54",
   "metadata": {},
   "source": [
    "<img src=\"../../assets/images/temperature.png\" width=\"70%\">\n",
    "\n",
    "*(Source: https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/04_parameters.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0fa43d5-2774-4a52-85d2-a741bb1ce10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration: we make three requests to Claude, asking it to \"Come up with a name for an alien planet. Respond with a single word.\"\n",
    "# first using a temperature of 0 and then a temperature of 1\n",
    "def demonstrate_temperatures():\n",
    "    temperatures = [0, 1]\n",
    "    for temperature in temperatures:\n",
    "        print(f\"Prompting Clause three times with a temperaturte of {temperature}:\")\n",
    "        print(\"================\")\n",
    "        for i in range(3):\n",
    "            response = client.messages.create(\n",
    "                model=MODEL_NAME,\n",
    "                max_tokens=100,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Come up with a name for an alien planet. Respond with a single word.\"}],\n",
    "                # messages=[{\"role\": \"user\", \"content\": \"Come up with a name for a cat. Respond with a single word.\"}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            print(f\"Response {i+1}: {response.content[0].text}\")\n",
    "        print(\"================\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b8a0ee1-478a-48c6-92fb-52231b82c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting Clause three times with a temperaturte of 0:\n",
      "================\n",
      "Response 1: Kestrax\n",
      "Response 2: Kestrax\n",
      "Response 3: Kestrax\n",
      "================\n",
      "Prompting Clause three times with a temperaturte of 1:\n",
      "================\n",
      "Response 1: Kestrax\n",
      "Response 2: Kelaryx\n",
      "Response 3: Xelara\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "demonstrate_temperatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f0dcf-7bb9-4187-a290-987380c3f0fb",
   "metadata": {},
   "source": [
    "Another example, illustrated with a chart. As an experiment Clause was queried 100 times with the prompt, \"Pick any animal in the world. Respond with only a single word: the name of the animal,\".\n",
    "\n",
    "When queried with a temperature of 0, Claude responded with \"Giraffe\" every single time.\n",
    "\n",
    "With a temperature of 1 the responses also include different types of animals (although giraffe is chosen more than half the time).\n",
    "\n",
    "<img src=\"../../assets/images/temperature_plot.png\" width=\"70%\" />\n",
    "\n",
    "*(Source: https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/04_parameters.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cfb50-5656-46ea-b36b-81b5c34128eb",
   "metadata": {},
   "source": [
    "### System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1619bdc-8e48-4938-b6cb-a55e64377872",
   "metadata": {},
   "source": [
    "The `system_prompt` (optional parameter) sets the stage for the conversation by giving Claude high-level instructions like defining its role or providing background information that should inform its responses.\n",
    "\n",
    "Key points about the system_prompt:\n",
    "\n",
    "- It's optional but can be useful for setting the tone and context of the conversation.\n",
    "- It's applied at the conversation level, affecting all of Claude's responses in that exchange.\n",
    "- It can help steer Claude's behavior without needing to include instructions in every user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1cdc3629-0f0a-4ec7-9870-2bfc17930160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo! Ich freue mich, Ihnen heute zu begegnen. Wie kann ich Ihnen heute helfen?\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    system=\"You are a helpful foreign language tutor that always responds in German.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hey there, how are you?!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd36de-6718-4a4d-9f12-1e22366701cd",
   "metadata": {},
   "source": [
    "### Excercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f6722cc-b01a-4e4f-8491-fa02ba0be470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(topic, num_questions):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=999,\n",
    "        system=f\"You are a helpful expert on the topic {topic} and you return your response as a numbered list.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Please generate thought-provoking questions about the topic {topic}\"}\n",
    "        ],\n",
    "        stop_sequences=[f\"{num_questions + 1}.\"]\n",
    "    )\n",
    "    \n",
    "    print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f40e5956-db55-4c75-ba6a-632f6d3483b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If all our actions are influenced by prior causes (genetics, upbringing, circumstances), can any choice truly be \"free\"?\n",
      "\n",
      "2. How does the existence of unconscious brain activity that precedes conscious decisions affect our understanding of free will?\n",
      "\n",
      "3. If we could perfectly predict someone's actions based on brain activity, would that disprove free will?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(topic=\"free will\", num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd60d2bf-3642-49ab-b5a7-e330c68a66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Is Universal Basic Income (UBI) a sustainable solution for economic inequality, or would it lead to inflation?\n",
      "\n",
      "2. How does the concept of \"free money\" through government stimulus checks impact individual work ethic?\n",
      "\n",
      "3. What are the psychological effects of receiving unexpected windfall money versus earned income?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(topic=\"free money\", num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5c7edb8-0a8f-49fc-bb08-6bd788ff1ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How has Jupyter Notebook revolutionized the way data scientists collaborate and share their work compared to traditional programming environments?\n",
      "\n",
      "2. What are the potential security implications of running Jupyter Notebooks in enterprise environments, and how can these risks be mitigated?\n",
      "\n",
      "3. How does the integration of markdown and code cells in Jupyter Notebook influence the way we approach documentation and literate programming?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(topic=\"jupyter notebook\", num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d866c-cd3e-4a98-8eca-5ff2fc2672ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
