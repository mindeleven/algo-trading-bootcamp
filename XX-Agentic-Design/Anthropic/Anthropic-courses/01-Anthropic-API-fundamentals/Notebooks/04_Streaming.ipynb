{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f850a16-be9d-4151-9c2c-cf8ce009398a",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7415415-1ec4-49c2-8065-e8f320d9b8cd",
   "metadata": {},
   "source": [
    "*(Coding along with the [Anthropic API fundamentals](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals) of Anthropic's courses GitHub repo)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d9dd4-d2a1-4d3c-b7d0-c0024ffe2aef",
   "metadata": {},
   "source": [
    "The goals of this section are to understand how streaming works and to work with stream events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b835bb9-76fd-4e1c-b500-30e4b331006b",
   "metadata": {},
   "source": [
    "## Basis setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d913e369-6dc6-4de9-b80b-5aac69eb8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/05_Streaming.ipynb\n",
    "from anthropic import Anthropic\n",
    "import pandas as pd\n",
    "\n",
    "anthropic_api_key = pd.read_csv(\"~/tmp/anthropic/anthropic-key-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")\n",
    "\n",
    "# instantiating the client\n",
    "client = Anthropic(api_key=anthropic_api_key)\n",
    "MODEL_NAME=\"claude-3-5-sonnet-20241022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8200e6-be01-4b1c-bd68-f17fe9ea9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a response back!\n",
      "========================\n",
      "Here's an essay about macaws and clay licks in the Amazon:\n",
      "\n",
      "Macaws and Clay Licks: A Fascinating Natural Phenomenon in the Amazon\n",
      "\n",
      "Deep within the Amazon rainforest, one of nature's most spectacular displays occurs daily at clay licks, where hundreds of brilliant scarlet macaws, blue-and-yellow macaws, and other parrot species gather to consume clay from exposed riverbank walls. This remarkable behavior, known as geophagy, is not merely a curious habit but rather a vital aspect of these birds' survival strategy.\n",
      "\n",
      "Clay licks, locally known as \"collpas,\" are natural clay banks typically found along river edges throughout the Amazon basin, particularly in Peru, Brazil, and Ecuador. These sites serve as essential mineral supplementation stations for various species of parrots, but macaws are perhaps the most striking visitors due to their large size and vibrant plumage.\n",
      "\n",
      "The primary reason macaws engage in clay consumption is to neutralize toxic compounds found in their diet. These magnificent birds feed on a variety of seeds, fruits, and nuts, many of which contain harmful alkaloids and tannins. The clay's high sodium content and its ability to bind with toxins help protect the birds' digestive systems from these potentially dangerous substances. Additionally, the clay provides essential minerals such as calcium, which is particularly important for breeding females.\n",
      "\n",
      "The daily gathering at clay licks follows a predictable pattern. In the early morning hours, macaws begin to congregate in nearby trees, creating a cacophony of squawks and calls. This preliminary gathering serves multiple purposes: it allows the birds to assess potential threats and creates safety in numbers. Once comfortable, they descend in groups to the clay bank, creating an extraordinary spectacle of color against the earthen backdrop.\n",
      "\n",
      "These clay lick sites have become increasingly important for Amazon conservation efforts. They serve as crucial focal points for protecting not only macaws but entire ecosystems. Because clay licks are fixed locations that birds regularly visit, they provide excellent opportunities for scientific research and wildlife monitoring. Researchers can study population dynamics, breeding success, and the overall health of macaw populations.\n",
      "\n",
      "However, these essential natural resources face various threats. Human activities such as logging, mining, and tourism can disturb the delicate balance of these sites. When macaws feel threatened, they may abandon traditional clay licks, potentially impacting their health and reproductive success. Conservation organizations work diligently to protect these areas, often establishing buffer zones and regulated viewing areas that allow for sustainable ecotourism while minimizing disruption to the birds.\n",
      "\n",
      "The relationship between macaws and clay licks also highlights the intricate connections within rainforest ecosystems. The presence of healthy clay licks often indicates a well-functioning forest system, as these sites require specific geological and hydrological conditions to form and persist. The regular gathering of macaws attracts other wildlife, creating complex ecological interactions that scientists are still working to understand fully.\n",
      "\n",
      "For indigenous communities in the Amazon, clay licks have long held cultural significance. Many traditional beliefs and practices are associated with these sites, and local knowledge has contributed significantly to our understanding of their importance. Today, some indigenous communities participate in conservation efforts and eco-tourism initiatives centered around clay licks, providing sustainable economic opportunities while protecting these crucial natural resources.\n",
      "\n",
      "The future of Amazon clay licks and their magnificent macaw visitors depends on continued conservation efforts and sustainable management. As we learn more about the complex relationships between these birds and their environment, it becomes increasingly clear that protecting clay licks is essential not only for the survival of macaw populations but for maintaining the biodiversity and ecological balance of the Amazon rainforest as a whole.\n",
      "\n",
      "In conclusion, the phenomenon of macaws visiting clay licks represents one of nature's most fascinating examples of animal behavior and ecological adaptation. These sites serve as\n"
     ]
    }
   ],
   "source": [
    "# just as a recap, the syntax we've used so far:\n",
    "response = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write me an essay about macaws and clay licks in the Amazon\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=800,\n",
    "    temperature=0,\n",
    ")\n",
    "print(\"We have a response back!\")\n",
    "print(\"========================\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4226f0b-0c56-479c-89fc-018719e83cef",
   "metadata": {},
   "source": [
    "With the approach we just used, we only get content back from the API once all of the content has been generated. Nothing is printed out until the entire response is printed all at once. If we don't want to wait for the entire response to be generated before we get a response, we can use __streaming__. With this approach content is streamed to a user's browser and displayed while the model is generating the response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46e1c7-ac4f-4d1d-8e8c-378bb3d05319",
   "metadata": {},
   "source": [
    "### Working with streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efddfa0-d668-48c7-a125-b417bbe7ca38",
   "metadata": {},
   "source": [
    "To get a streaming response from the API we've to pass `stream=True` as a parameter to `client.messages.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69de37e-61f7-4f6f-9686-396189e3c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee491ee-d2cf-4be2-8d7d-98e05301b874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<anthropic.Stream at 0x112273f80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at the stream variable\n",
    "stream # stream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af22b516-bd1e-4b13-8f27-d8042763d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01WP4BHRURLro5RBcLwciaAZ', content=[], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=30, output_tokens=1)), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Dogs', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' chase cats.', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(output_tokens=7))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "# the stream object is a generator object that yields individual server-sent events (SSE) as they are received from the API\n",
    "# we need to iterate over the stream object and work with each individual server-sent event\n",
    "# iterating over the stream response:\n",
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41156db2-ed98-40c8-8cf0-82f9fa665c26",
   "metadata": {},
   "source": [
    "#### __Color-coded explanation of the streaming events:__\n",
    "\n",
    "<img src=\"../../assets/images/streaming_events.png\" width=\"70%\" />\n",
    "\n",
    "Each stream contains a series of events in the following order:\n",
    "\n",
    "- __MessageStartEvent__ - A message with empty content\n",
    "- __Series of content blocks__ - Each of which contains:\n",
    "  - A __ContentBlockStartEvent__\n",
    "  - One or more __ContentBlockDeltaEvents__\n",
    "  - A __ContentBlockStopEvent__\n",
    "- One or more __MessageDeltaEvents__ which indicate top-level changes to the final message\n",
    "- A final __MessageStopEvent__\n",
    "\n",
    "#### __Events associated with a single content block:__\n",
    "\n",
    "<img src=\"../../assets/images/content_block_events.png\" width=\"70%\" />\n",
    "\n",
    "As we can see in the diagram, the model-generated content we care about comes from the ContentBlockDeltaEvents. Each of them contains a type set to \"content_block_delta.\" To get the content itself, we need to access the `text` property inside of `delta`.\n",
    "\n",
    "*(Image source: # https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/05_Streaming.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984791ac-1b2e-4465-a41d-f1b7fb2fc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs\n",
      " chase cats.\n"
     ]
    }
   ],
   "source": [
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == \"content_block_delta\":\n",
    "        print(event.delta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99aa7f-361e-499f-a764-df4b98ce7c5e",
   "metadata": {},
   "source": [
    "To format the printed content a bit more nicely, we can make use of two additional arguments that we pass to `print()`:\n",
    "\n",
    "- `end=\"\"`: By default, the print() function adds a newline character (\\n) at the end of the printed text. However, by setting end=\"\", we specify that the printed text should not be followed by a newline character. This means that the next print() statement will continue printing on the same line.\n",
    "- `flush=True`: The flush parameter is set to True to force the output to be immediately written to the console or standard output, without waiting for a newline character or the buffer to be filled. This ensures that the text is displayed in real-time as it is received from the streaming response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19efa44d-d355-4223-95d8-02c6d685a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs chase cats."
     ]
    }
   ],
   "source": [
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    if event.type == \"content_block_delta\":\n",
    "        print(event.delta.text, flush=True, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b576ffd6-c2cd-40c7-9131-ca5061febe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) work through several key components and processes. Here's a simplified explanation:\n",
      "\n",
      " Process:ng\n",
      "d on massive amounts of text data from the internet, books, and other sources\n",
      " patterns in language through a process called \"unsupervised learning\"\n",
      " involves predicting the next word in a sequence based on previous words\n",
      "\n",
      "2. Key Components:\n",
      " Transformer architecture: The underlying neural network structure\n",
      ": Help the model focus on relevant parts of input text\n",
      " of adjustable values that store learned patterns\n",
      "\n",
      "3. Basic Operation:\n",
      " text input\n",
      " multiple layers of neural networks\n",
      "d patterns to generate appropriate responses\n",
      "dicts most likely next words based on context\n",
      "\n",
      ":. Key Capabilities\n",
      "- Pattern recognition in language\n",
      "- Understanding context\n",
      "- Text generation\n",
      " adaptation\n",
      "\n",
      "5. Limitations:\n",
      "- No true understanding or consciousness\n",
      " incorrect information\n",
      "off dated to training data cut\n",
      "- No real-time information\n",
      "\n",
      " the actual technical details are quite complex and involve advanced mathematics and computer science concepts."
     ]
    }
   ],
   "source": [
    "# now for a longer text response\n",
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    if event.type == \"content_block_delta\":\n",
    "        print(event.delta.text, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec890560-3bb1-44c4-a68a-dcc4b4b183ed",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Interlude: A brief conversation with Claude.ai</span>\n",
    "\n",
    ">Hello Claude! I'm working with the streaming example of the Claude Python SDK. \n",
    ">\n",
    ">I'm using the following example code on a M1 Mac:\n",
    ">\n",
    ">```\n",
    ">stream = client.messages.create(\n",
    ">    messages=[\n",
    ">        {\n",
    ">            \"role\": \"user\",\n",
    ">           \"content\": \"How do large language models work?\",\n",
    ">        }\n",
    ">    ],\n",
    ">    model=MODEL_NAME,\n",
    ">    max_tokens=100,\n",
    ">    temperature=0,\n",
    ">    stream=True,\n",
    ">)\n",
    ">for event in stream:\n",
    ">    if event.type == \"content_block_delta\":\n",
    ">        print(event.delta.text, flush=True, end=\"\")\n",
    ">```\n",
    "><br>\n",
    ">\n",
    ">In the printout that I'm getting the first characters of every paragraph (except for the first one) are missing. I assume that the problem is in the code line `print(event.delta.text, flush=True, end=\"\")`. Do you have suggestions how to modify the code to get a correct output?    print(text_buffer, end=\"\")\n",
    "\n",
    "__Claude.ai:__ This is a common issue with streaming outputs where newline characters and the first characters after them can sometimes get separated across different chunks. Here's how you can modify the code to handle this better:\n",
    "\n",
    "```python\n",
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Buffer to store accumulated text\n",
    "text_buffer = \"\"\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == \"content_block_delta\":\n",
    "        chunk = event.delta.text\n",
    "        text_buffer += chunk\n",
    "        \n",
    "        # If we have a complete line (ends with newline), print it\n",
    "        while '\\n' in text_buffer:\n",
    "            line, text_buffer = text_buffer.split('\\n', 1)\n",
    "            print(line)\n",
    "            print()  # Add blank line after each paragraph\n",
    "            \n",
    "# Print any remaining text in the buffer\n",
    "if text_buffer:\n",
    "    print(text_buffer, end=\"\")\n",
    "```\n",
    "\n",
    "The key changes are:\n",
    "1. We create a text buffer to accumulate chunks\n",
    "2. We only print complete lines when we encounter a newline character\n",
    "3. We split the buffer at newlines and keep the remainder for the next iteration\n",
    "4. At the end, we print any remaining text that didn't end with a newline\n",
    "\n",
    "This approach ensures that paragraphs are printed completely and correctly, without missing any characters. The streaming chunks will be reassembled properly before being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35660a76-6c1e-4d3c-b81d-43620eab84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) work through several key components and processes. Here's a simplified explanation:\n",
      "\n",
      "1. Architecture:\n",
      "- LLMs are based on transformer architecture, which uses self-attention mechanisms\n",
      "- They consist of billions of parameters (weights and biases) arranged in neural networks\n",
      "- The models are typically organized in multiple layers that process information sequentially\n",
      "\n",
      "2. Training:\n",
      "- Models are trained on massive amounts of text data from the internet, books, and other sources\n",
      "- They learn patterns in language through unsupervised learning\n",
      "- Training involves predicting the next word in a sequence, helping them understand context and relationships\n",
      "\n",
      "3. Key processes:\n",
      "- Tokenization: Breaking input text into smaller units (tokens)\n",
      "- Attention mechanisms: Weighing the importance of different words in context\n",
      "- Pattern recognition: Identifying relationships between words and concepts\n",
      "- Probability distribution: Generating likely responses based on training data\n",
      "\n",
      "4. Limitations:\n",
      "- No true understanding or consciousness\n",
      "- Can produce incorrect or biased information\n",
      "- Limited to training data cutoff date\n",
      "- No real-time information access\n",
      "\n",
      "5. Applications:\n",
      "- Text generation\n",
      "- Translation\n",
      "- Summarization\n",
      "- Question-answering\n",
      "- Code generation\n",
      "- Content creation\n",
      "\n",
      "This is a high-level overview; the actual technical details are much more complex."
     ]
    }
   ],
   "source": [
    "# code example generated by claude.ai:\n",
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Buffer to store accumulated text\n",
    "text_buffer = \"\"\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == \"content_block_delta\":\n",
    "        chunk = event.delta.text\n",
    "        text_buffer += chunk\n",
    "        \n",
    "        # If we have a complete line (ends with newline), print it\n",
    "        while '\\n' in text_buffer:\n",
    "            line, text_buffer = text_buffer.split('\\n', 1)\n",
    "            print(line)\n",
    "            # print()  # Add blank line after each paragraph\n",
    "            \n",
    "# Print any remaining text in the buffer\n",
    "if text_buffer:\n",
    "    print(text_buffer, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87e6171-48f7-473f-82d6-fba06a9faaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) work through several key components and processes. Here's a simplified explanation:\n",
      "\n",
      "1. Training Process:\n",
      "- LLMs are trained on massive amounts of text data from the internet, books, and other sources\n",
      "- They learn patterns in language through a process called \"unsupervised learning\"\n",
      "- Training involves predicting the next word in a sequence based on previous words\n",
      "\n",
      "2. Key Components:\n",
      "- Transformer architecture: The underlying neural network structure that processes text\n",
      "- Attention mechanisms: Allow the model to focus on relevant parts of input text\n",
      "- Parameters: Billions of adjustable values that store learned patterns\n",
      "\n",
      "3. Basic Operation:\n",
      "- Takes text input (prompt)\n",
      "- Processes it through multiple layers of neural networks\n",
      "- Generates predictions for the most likely next words\n",
      "- Outputs coherent text based on learned patterns\n",
      "\n",
      "4. Key Capabilities:\n",
      "- Pattern recognition in language\n",
      "- Understanding context\n",
      "- Text generation\n",
      "- Task adaptation through prompting\n",
      "\n",
      "5. Limitations:\n",
      "- No true understanding or consciousness\n",
      "- Can produce incorrect information\n",
      "- Limited to training data cutoff date\n",
      "- No real-time information access\n",
      "\n",
      "This is a simplified overview - the actual technical details are quite complex and involve advanced mathematics and computer science concepts."
     ]
    }
   ],
   "source": [
    "# claude's a concise and elegant version with the streaming logic separated into its own function\n",
    "def print_stream_response(stream):\n",
    "    buffer = \"\"\n",
    "    for event in stream:\n",
    "        if event.type == \"content_block_delta\":\n",
    "            buffer += event.delta.text\n",
    "            while '\\n' in buffer:\n",
    "                line, buffer = buffer.split('\\n', 1)\n",
    "                # print(line, '\\n')\n",
    "                print(line)\n",
    "    if buffer:\n",
    "        print(buffer, end=\"\")\n",
    "\n",
    "# API call\n",
    "stream = client.messages.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do large language models work?\"}],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print_stream_response(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2db9a8-a12e-4774-9218-d873f847f3cc",
   "metadata": {},
   "source": [
    "In this modified version Claude has made the following __improvements__:\n",
    "\n",
    "- Moved the streaming logic into a dedicated function for better organization\n",
    "- Simplified variable names (text_buffer → buffer)\n",
    "- Removed redundant comments\n",
    "- Condensed the messages list into a single line\n",
    "- Combined the print statements for the line and newline\n",
    "\n",
    "The function remains robust but is now more concise and reusable. You can easily use it for any streaming response from the Claude API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd1ce68-7ec0-46da-a925-2f31757b00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01KdyNHrwqmq9jd4woh5wcbR', content=[], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=14, output_tokens=2)), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Large Language', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' Models (LLMs) work through', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=\" several key components and processes. Here's a\", type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' simplified explanation:\\n\\n1. Architecture', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=':\\n- LLMs are base', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='d on transformer architecture, which uses self-', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='attention mechanisms\\n- They consist of billions', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' of parameters (weights and biases) arrange', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='d in neural networks\\n- The models', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' are typically organized in multiple layers that', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' process information sequentially\\n\\n2.', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' Training:\\n- Models are trained on massive', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' amounts of text data from the internet,', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' books, and other sources\\n- They', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' learn patterns in language through uns', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='upervised learning\\n- Training involves pre', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='dicting the next word in a sequence,', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' helping the model understand context and relationships', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='\\n\\n3. Key processes:\\n-', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' Tokenization: Breaking input', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' text into smaller units (tokens)', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='\\n- Pattern recognition: Identifying relationships', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' between words and concepts\\n- Context', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='ual understanding: Processing the meaning of words based on surrounding', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' context\\n- Probability-', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='based prediction: Generating responses based on learned patterns', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='\\n\\n4. Limitations:\\n- No', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' true understanding or consciousness\\n- Can', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' produce incorrect or biased information', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='\\n- Limited to training data cut', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='off date\\n- No', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' real-time internet access or', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' ability to learn from conversations', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='\\n\\n5. Applications:\\n- Text', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' generation\\n- Translation\\n-', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' Summarization\\n- Question-', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='answering\\n- Code generation\\n- Content', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' creation\\n\\nThis is a high-level', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' overview; the actual technical details are much more complex', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='.', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(output_tokens=296))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "# once again above example with printing out the whole event\n",
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2e8eb1a-77a6-4fd2-8bc1-39303592f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) work through several key components and processes. Here's a simplified explanation:\n",
      "\n",
      " Process:ng\n",
      "d on massive amounts of text data from the internet, books, and other sources\n",
      "upervised learning\"ns in language through a process called \"uns\n",
      "dicting the next word in a sequence based on previous words\n",
      "\n",
      "2. Key Components:\n",
      " architecture: The underlying neural network structure\n",
      " Attention mechanisms: Help the model focus on relevant parts of input text\n",
      " of adjustable values that store learned patterns\n",
      "\n",
      " Operation:\n",
      "- Takes text input\n",
      " multiple layers of neural networks\n",
      "d patterns to generate appropriate responses\n",
      "dicts most likely next words based on context\n",
      "\n",
      " Key Capabilities:\n",
      " language recognition in\n",
      "- Understanding context\n",
      " generation\n",
      "- Task adaptation\n",
      "\n",
      "5. Limitations:\n",
      "- No true understanding or consciousness\n",
      " incorrect or biased information\n",
      "d to training data cutoff date\n",
      " real-time information\n",
      "\n",
      "d overview - the actual technical details are quite complex and involve advanced mathematics and computer science concepts."
     ]
    }
   ],
   "source": [
    "# alternative approach with the same result\n",
    "with client.messages.stream(\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,\n",
    ") as stream:\n",
    "  for text in stream.text_stream:\n",
    "      print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f319328-ec64-438c-947d-6cc1b7419c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresponse = client.messages.create(\\n    messages=[\\n        {\\n            \"role\": \"user\",\\n            \"content\": \"How do large language models work?\",\\n        }\\n    ],\\n    model=MODEL_NAME,\\n    max_tokens=1000,\\n    temperature=0,\\n)\\nprint(response.content[0].text)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "response = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    ")\n",
    "print(response.content[0].text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1981cb-ee5c-47ea-b7dd-1a5c8171fe9b",
   "metadata": {},
   "source": [
    "#### __Accessing information about our token usage:__\n",
    "\n",
    "- `MessageStartEvent` contains our input(prompt) token usage information\n",
    "- `MessageDeltaEvent` contains information on how many output tokens were generated\n",
    "\n",
    "<img src=\"../../assets/images/events_token_usage.png\" width=\"70%\" />\n",
    "\n",
    "*(Image source: # https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/05_Streaming.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf988421-91c6-45a0-bcfb-0ce863f1bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSAGE START EVENT\n",
      "Input tokens used: 14\n",
      "========================\n",
      " Models (LLMs) work through several key components and processes. Here's a simplified explanation:\n",
      "\n",
      ":. Architecture\n",
      "d on transformer architecture, which uses attention mechanisms\n",
      "d biases)nsist of billions of parameters (weights an\n",
      " sequences of tokensas\n",
      "\n",
      ":. Training Process\n",
      " amounts of text datasive\n",
      " patterns and relationships between words/tokens\n",
      " self-supervised learning to predict next words in sequences\n",
      " through backpropagation\n",
      "\n",
      "3. Key Features:\n",
      " mechanisms to focus on relevant parts of input\n",
      " Context understanding through positional encoding\n",
      " recognition across different levels of language\n",
      "\n",
      ":. Operation\n",
      "split into smaller units) (\n",
      " transformer layersrough multiple\n",
      "d on learned patternss base\n",
      " distributions for next tokens\n",
      "\n",
      ":. Capabilities\n",
      "- Text completion\n",
      "- Question answering\n",
      "- Translation\n",
      "- Summarization\n",
      " generation\n",
      "- General language understanding\n",
      "\n",
      " is a simplified overview - the actual implementation involves complex mathematics and computational processes.\n",
      "========================\n",
      "MESSAGE DELTA EVENT\n",
      "Output tokens used: 245\n"
     ]
    }
   ],
   "source": [
    "# printing out how many tokens are used in our prompt and how many tokens the model generates:\n",
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do large language models work?\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=1000,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    if event.type == \"message_start\":\n",
    "        input_tokens = event.message.usage.input_tokens\n",
    "        print(\"MESSAGE START EVENT\", flush=True)\n",
    "        print(f\"Input tokens used: {input_tokens}\", flush=True)\n",
    "        print(\"========================\")\n",
    "    elif event.type == \"content_block_delta\":\n",
    "        print(event.delta.text, flush=True, end=\"\")\n",
    "    elif event.type == \"message_delta\":\n",
    "        output_tokens = event.usage.output_tokens\n",
    "        print(\"\\n========================\", flush=True)\n",
    "        print(\"MESSAGE DELTA EVENT\", flush=True)\n",
    "        print(f\"Output tokens used: {output_tokens}\", flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860850a-6057-46c2-b063-225fd77fe77c",
   "metadata": {},
   "source": [
    "#### Other streaming event types\n",
    "\n",
    "\n",
    "- __Ping events__: Streams may also include any number of ping events.\n",
    "- __Error events__: Error events may occasionally occur in the event stream. During periods of high usage you may receive an overloaded_error (corresponds to a HTTP 529 in a non-streaming context).\n",
    "\n",
    "Error event example:\n",
    "```\n",
    "event: error\n",
    "data: {\"type\": \"error\", \"error\": {\"type\": \"overloaded_error\", \"message\": \"Overloaded\"}}\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a77e6-9e53-467f-978c-4e71dff3f4e3",
   "metadata": {},
   "source": [
    "### Time to first token (TTFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868679a-62f1-405e-9ad2-eaed91b0c7cc",
   "metadata": {},
   "source": [
    "Streaming is supposed to improve the amount of time it takes for us to receive the first bit of model generated content - the __Time to first token (TTFT)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2c6e3fa-7d22-4270-8907-2a3f433e262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) non-streaming approach, asking the model to generate a very long piece of text but cut it off at 500 tokens\n",
    "import time\n",
    "def measure_non_streaming_ttft():\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.messages.create(\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        model=MODEL_NAME,\n",
    "    )\n",
    "\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Time to receive first token: {response_time:.3f} seconds\")\n",
    "    print(f\"Time to recieve complete response: {response_time:.3f} seconds\")\n",
    "    print(f\"Total tokens generated: {response.usage.output_tokens}\")\n",
    "    \n",
    "    print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38fba54-30f1-4ce7-b746-0bb5b702a8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to receive first token: 9.874 seconds\n",
      "Time to recieve complete response: 9.874 seconds\n",
      "Total tokens generated: 500\n",
      "Here's a comprehensive essay on the American Revolution:\n",
      "\n",
      "The American Revolution (1765-1783) was a pivotal moment in world history that led to the creation of the United States of America and marked the first successful colonial rebellion against a European power. The revolution was the result of growing tensions between Great Britain and its thirteen American colonies, ultimately leading to a full-scale war and the birth of a new nation.\n",
      "\n",
      "Origins and Early Tensions:\n",
      "The seeds of revolution were planted in the aftermath of the French and Indian War (1754-1763). Although Britain emerged victorious, the war left it with substantial debt. To recover these costs, Parliament began imposing new taxes on the American colonies. The British government believed that since they had protected the colonies during the war, the colonists should help pay for it.\n",
      "\n",
      "In 1764, the Sugar Act was passed, followed by the Stamp Act in 1765, which required colonists to pay a tax on virtually every piece of printed paper they used. The colonial response to these taxes was summarized by the slogan \"No taxation without representation,\" as colonists argued that they had no representatives in Parliament and therefore should not be taxed by it.\n",
      "\n",
      "Escalating Tensions:\n",
      "The situation continued to deteriorate with the passage of the Townshend Acts in 1767, which imposed new taxes on common goods like tea, paper, and glass. Colonial resistance took the form of boycotts, protests, and the formation of groups like the Sons of Liberty. The Boston Massacre in 1770, where British soldiers killed five colonists, further inflamed anti-British sentiment.\n",
      "\n",
      "In 1773, the Tea Act sparked one of the most famous protests in American history - the Boston Tea Party. Colonists, disguised as Native Americans, dumped 342 chests of British tea into Boston Harbor in protest. The British Parliament responded with the Coercive Acts (called the Intolerable Acts by the colonists), which included closing Boston Harbor and restricting Massachusetts' self-governance.\n",
      "\n",
      "The Road to Revolution:\n",
      "In response to British actions, the colonies convened the First Continental Congress in 1774 to coordinate their resistance. When British troops attempted to seize colonial weapons and arrest revolutionary leaders in April 1775, the first shots of the revolution were fired at Lexington and Concord, Massachusetts.\n"
     ]
    }
   ],
   "source": [
    "measure_non_streaming_ttft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3877bb21-c7c6-4361-ab78-2458f82147fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing using a streaming approach\n",
    "def measure_streaming_ttft():\n",
    "    start_time = time.time()\n",
    "\n",
    "    stream = client.messages.create(\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        stream=True\n",
    "    )\n",
    "    have_received_first_token = False\n",
    "    for event in stream:\n",
    "        if event.type == \"content_block_delta\":\n",
    "            if not have_received_first_token:\n",
    "                ttft = time.time() - start_time\n",
    "                have_received_first_token = True\n",
    "            print(event.delta.text, flush=True, end=\"\")\n",
    "        elif event.type == \"message_delta\":\n",
    "            output_tokens = event.usage.output_tokens\n",
    "            total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nTime to receive first token: {ttft:.3f} seconds\", flush=True)\n",
    "    print(f\"Time to recieve complete response: {total_time:.3f} seconds\", flush=True)\n",
    "    print(f\"Total tokens generated: {output_tokens}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97748efd-3d13-422e-8e3e-56fb79ab2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a long essay explaining the history of the American Revolution:\n",
      "\n",
      " history of the United States, marking the country's transition from a collection of British colonies to an independent nation. The roots of the revolution can be traced back to the French and Indian War, which was fought between Britain and France from 1754 to 1763. This conflict resulted in the British gaining control of much of North America, but it also left the British government with a significant debt that they sought to recoup by imposing new taxes and regulations on the colonies.\n",
      "\n",
      " major point of contention between the colonies and the British government was the Stamp Act, which was passed in 1765. This act required all printed materials in the colonies, including newspapers, pamphlets, bills, legal documents, licenses, almanacs, dice, and playing cards, to carry an embossed revenue stamp. The colonists viewed this as a violation of their rights as British subjects, as they had no representation in the British Parliament and therefore no say in the taxes that were being imposed on them.\n",
      "\n",
      "d to the Stamp Act with widespread protests and boycotts, and the act was eventually repealed in 1766. However, this was just the beginning of a series of increasingly contentious actions taken by the British government, including the Townshend Acts, which imposed new taxes on imported goods, and the Boston Massacre, in which British soldiers fired on a crowd of protesters, killing five people.\n",
      "\n",
      "d to revolution came in 1773, when the British East India Company was granted a monopoly on the tea trade in the colonies. In response, a group of colonists in Boston, known as the Sons of Liberty, boarded a British ship and dumped hundreds of chests of tea into the harbor, an event that became known as the Boston Tea Party.\n",
      "\n",
      " responded to the Boston Tea Party by passing a series of punitive laws known as the Intolerable Acts, which further angered the colonists and led to the convening of the First Continental Congress in 1774. This body, made up of representatives from the thirteen colonies, issued a declaration of rights and grievances and called for the colonies to prepare for war.\n",
      "\n",
      " shots of the American Revolution were fired in April 1775, when British troops marche\n",
      "Time to receive first token: 0.326 seconds\n",
      "Time to recieve complete response: 3.928 seconds\n",
      "Total tokens generated: 500\n"
     ]
    }
   ],
   "source": [
    "measure_streaming_ttft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b37b9-2d17-4086-84aa-f16d35a79bac",
   "metadata": {},
   "source": [
    "Comparing the results:\n",
    "\n",
    "- __Without Streaming:__\n",
    "  - Time to receive first token: 8.193 seconds\n",
    "  - Time to recieve complete response: 8.193 seconds\n",
    "  - Total tokens generated: 500\n",
    "- __With Streaming:__\n",
    "  - Time to receive first token: 0.492 seconds\n",
    "  - Time to recieve complete response: 4.127 seconds\n",
    "  - Total tokens generated: 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfbf1c29-b314-4066-99db-6847b97cd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try an example that generates 1000 tokens using Opus\n",
    "def compare_ttft():\n",
    "    def measure_streaming_ttft():\n",
    "        start_time = time.time()\n",
    "\n",
    "        stream = client.messages.create(\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            stream=True\n",
    "        )\n",
    "        have_received_first_token = False\n",
    "        for event in stream:\n",
    "            if event.type == \"content_block_delta\":\n",
    "                if not have_received_first_token:\n",
    "                    ttft = time.time() - start_time\n",
    "                    have_received_first_token = True\n",
    "            elif event.type == \"message_delta\":\n",
    "                output_tokens = event.usage.output_tokens\n",
    "                total_time = time.time() - start_time\n",
    "        return (ttft, output_tokens)\n",
    "    \n",
    "    def measure_non_streaming_ttft():\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = client.messages.create(\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=\"claude-3-opus-20240229\"\n",
    "        )\n",
    "        ttft = time.time() - start_time\n",
    "        return (ttft, response.usage.output_tokens)\n",
    "    \n",
    "    streaming_ttft, streaming_tokens = measure_streaming_ttft()\n",
    "    non_streaming_ttft, non_streaming_tokens = measure_non_streaming_ttft()\n",
    "\n",
    "    print(\"OPUS STREAMING\")\n",
    "    print(f\"Time to first token: {streaming_ttft}\")\n",
    "    print(f\"Tokens generated: {streaming_tokens}\")\n",
    "    print(\"#########################################################\")\n",
    "    print(\"OPUS NON STREAMING\")\n",
    "    print(f\"Time to first token: {non_streaming_ttft}\")\n",
    "    print(f\"Tokens generated: {non_streaming_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10642cce-ec99-4d47-b074-d5321e58ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPUS STREAMING\n",
      "Time to first token: 0.7362079620361328\n",
      "Tokens generated: 1000\n",
      "#########################################################\n",
      "OPUS NON STREAMING\n",
      "Time to first token: 35.947712898254395\n",
      "Tokens generated: 1000\n"
     ]
    }
   ],
   "source": [
    "compare_ttft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953fb86-bedb-4f2e-a6bc-36fc124e3c38",
   "metadata": {},
   "source": [
    "### Streaming helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0a383e-0a74-420d-8a13-a65736d5fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In gardens fair, where beauty reigns supreme,\n",
      ", a queen among the blooms,\n",
      ", a sight that's sure to gleam,\n",
      "'s most exquisite plumes.\n",
      "\n",
      " rich, from pure white to deep red,\n",
      "d patterns intricate, beyond compare,\n",
      " blossom seems to have a story read,\n",
      " grace and charm beyond repair.\n",
      "\n",
      " fairies in the breeze,\n",
      " presence grand and strong,\n",
      " dancing to the hum of busy bees,\n",
      " the gentle birds' melodious song.\n",
      "\n",
      " orchid, how your beauty does beguile,\n",
      " in our sight.e, forever\n",
      "\n",
      "STREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \n",
      "{\n",
      "  \"id\": \"msg_01H885AdbChf5eZRynx61Jas\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"citations\": null,\n",
      "      \"text\": \"In gardens fair, where beauty reigns supreme,\\nThe orchid stands, a queen among the blooms,\\nHer petals soft, a sight that's sure to gleam,\\nAdorned in nature's most exquisite plumes.\\n\\nWith colors rich, from pure white to deep red,\\nAnd patterns intricate, beyond compare,\\nEach blossom seems to have a story read,\\nA tale of grace and charm beyond repair.\\n\\nSome delicate, like fairies in the breeze,\\nWhile others bold, with presence grand and strong,\\nAll dancing to the hum of busy bees,\\nAnd to the gentle birds' melodious song.\\n\\nOh orchid, how your beauty does beguile,\\nA treasure rare, forever in our sight.\",\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"claude-3-opus-20240229\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"stop_reason\": \"end_turn\",\n",
      "  \"stop_sequence\": null,\n",
      "  \"type\": \"message\",\n",
      "  \"usage\": {\n",
      "    \"cache_creation_input_tokens\": 0,\n",
      "    \"cache_read_input_tokens\": 0,\n",
      "    \"input_tokens\": 14,\n",
      "    \"output_tokens\": 168\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "client = AsyncAnthropic(api_key=anthropic_api_key)\n",
    "\n",
    "async def streaming_with_helpers():\n",
    "    async with client.messages.stream(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write me sonnet about orchids\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)        \n",
    "\n",
    "    final_message = await stream.get_final_message()\n",
    "    print(\"\\n\\nSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \")\n",
    "    print(final_message.to_json())\n",
    "\n",
    "await streaming_with_helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ce5492-4aeb-45f7-a022-dff54e30c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In gardens fair, where beauty's reign is true,\n",
      "There grows a flower of grace and elegance rare,\n",
      "With petals soft, of varied shape and hue,\n",
      "The orchid stands, a sight beyond compare.\n",
      "\n",
      "Some bloom in shades of purple, pink, or white,\n",
      "While others bear a pattern wild and bold,\n",
      "Each blossom is a masterpiece of light,\n",
      "A work of art, a wonder to behold.\n",
      "\n",
      "Their fragrance sweet, a gift from nature's hand,\n",
      "Enchants the senses, stirs the heart's desire,\n",
      "In forests deep or cultivated land,\n",
      "The orchid's charm will never cease to inspire.\n",
      "\n",
      "Oh, lovely orchid, symbol of refined taste,\n",
      "Your beauty shines, forever undefaced.\n",
      "\n",
      "STREAMING IS DONE. HERE IS THE FINAL ACCUMULATED MESSAGE:\n",
      "{\n",
      "  \"id\": \"msg_01NWc4ybWV7k5Tx3jf6h2Zi9\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"citations\": null,\n",
      "      \"text\": \"In gardens fair, where beauty's reign is true,\\nThere grows a flower of grace and elegance rare,\\nWith petals soft, of varied shape and hue,\\nThe orchid stands, a sight beyond compare.\\n\\nSome bloom in shades of purple, pink, or white,\\nWhile others bear a pattern wild and bold,\\nEach blossom is a masterpiece of light,\\nA work of art, a wonder to behold.\\n\\nTheir fragrance sweet, a gift from nature's hand,\\nEnchants the senses, stirs the heart's desire,\\nIn forests deep or cultivated land,\\nThe orchid's charm will never cease to inspire.\\n\\nOh, lovely orchid, symbol of refined taste,\\nYour beauty shines, forever undefaced.\",\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"claude-3-opus-20240229\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"stop_reason\": \"end_turn\",\n",
      "  \"stop_sequence\": null,\n",
      "  \"type\": \"message\",\n",
      "  \"usage\": {\n",
      "    \"cache_creation_input_tokens\": 0,\n",
      "    \"cache_read_input_tokens\": 0,\n",
      "    \"input_tokens\": 14,\n",
      "    \"output_tokens\": 170\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# once again with a little help from Claude:\n",
    "# here's how we can integrate the original print_stream_response function with the async example:\n",
    "async def print_stream_response(stream):\n",
    "    buffer = \"\"\n",
    "    # using uses client.messages.stream allows us to easily access generated text content as it streams in \n",
    "    # without having to manually check the stream event type\n",
    "    # stream.text_stream provides an iterator over just the text deltas in the stream\n",
    "    async for text in stream.text_stream:\n",
    "        buffer += text\n",
    "        while '\\n' in buffer:\n",
    "            line, buffer = buffer.split('\\n', 1)\n",
    "            print(line)\n",
    "    if buffer:\n",
    "        print(buffer, end=\"\")\n",
    "\n",
    "async def streaming_with_helpers():\n",
    "    # code uses client.messages.stream which allows us to use helpers like stream.text_stream (in print_stream_response())\n",
    "    # to easily access generated text content as it streams in without having to manually check the stream event type\n",
    "    async with client.messages.stream(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write me sonnet about orchids\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "    ) as stream:\n",
    "        await print_stream_response(stream)\n",
    "        # another useful helper methods is get_final_message \n",
    "        # which returns a final accumulated message once the stream has been read to completion\n",
    "        # get_final_message can be useful if you both want to use streaming \n",
    "        # but also need access to the entire finished text generation when it's complete\n",
    "        final_message = await stream.get_final_message()\n",
    "        print(\"\\n\\nSTREAMING IS DONE. HERE IS THE FINAL ACCUMULATED MESSAGE:\")\n",
    "        print(final_message.to_json())\n",
    "\n",
    "await streaming_with_helpers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2f22-f483-4fcc-8c25-0f03160f20c5",
   "metadata": {},
   "source": [
    "#### Passing an Event Handler argument to client.messages.stream\n",
    "\n",
    "The example that the documentation provides returns the following error: `TypeError: AsyncMessages.stream() got an unexpected keyword argument 'event_handler'`.  The recent Claude Python SDK seems to have changed how event handlers are implemented. The example I'm using here got rewritten with the help of Claude.ai.\n",
    "\n",
    "The code got modified to:\n",
    "1. Remove the event_handler parameter since it's no longer supported\n",
    "2. Directly process the message deltas in the stream loop\n",
    "3. Keep the color formatting for the output\n",
    "4. Still show different event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d8bad9-846c-4619-a9e3-b74b3318544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Event type: message_start\n",
      "\n",
      "Event type: content_block_start\n",
      "\u001b[32mWhispers echo through silent nights.\u001b[0m\n",
      "Event type: text\n",
      "\n",
      "Event type: content_block_stop\n",
      "\n",
      "Event type: message_delta\n",
      "\n",
      "Event type: message_stop\n",
      "\n",
      "\n",
      "Final accumulated message: {\n",
      "  \"id\": \"msg_01E4bAaADS2M2YFSbiRkwLnL\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"citations\": null,\n",
      "      \"text\": \"Whispers echo through silent nights.\",\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"claude-3-opus-20240229\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"stop_reason\": \"end_turn\",\n",
      "  \"stop_sequence\": null,\n",
      "  \"type\": \"message\",\n",
      "  \"usage\": {\n",
      "    \"cache_creation_input_tokens\": 0,\n",
      "    \"cache_read_input_tokens\": 0,\n",
      "    \"input_tokens\": 14,\n",
      "    \"output_tokens\": 11\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from anthropic import AsyncAnthropic, AsyncMessageStream\n",
    "\n",
    "class ColoredStream(AsyncMessageStream):\n",
    "    # ANSI color codes\n",
    "    green = '\\033[32m'\n",
    "    reset = '\\033[0m'\n",
    "    \n",
    "    async def on_text(self, text, snapshot):\n",
    "        # Print model's text in green\n",
    "        print(f\"{self.green}{text}{self.reset}\", end=\"\", flush=True)\n",
    "\n",
    "    async def on_stream_event(self, event):\n",
    "        # Handle different event types\n",
    "        if event.type == \"message_start\":\n",
    "            print(\"\\nMessage started!\")\n",
    "        elif event.type == \"content_block_start\":\n",
    "            print(\"\\nContent block started!\")\n",
    "        elif event.type == \"content_block_stop\":\n",
    "            print(\"\\nContent block stopped!\")\n",
    "        elif event.type == \"message_delta\":\n",
    "            print(f\"\\nMessage delta received\")\n",
    "        elif event.type == \"message_stop\":\n",
    "            print(\"\\nMessage completed!\")\n",
    "\n",
    "async def stream_with_handlers():\n",
    "    client = AsyncAnthropic(api_key=anthropic_api_key)\n",
    "    \n",
    "    async with client.messages.stream(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Generate a 5-word poem\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "    ) as stream:\n",
    "        async for message_delta in stream:\n",
    "            if message_delta.type == \"content_block_delta\":\n",
    "                print(f\"{ColoredStream.green}{message_delta.delta.text}{ColoredStream.reset}\", end=\"\", flush=True)\n",
    "            else:\n",
    "                print(f\"\\nEvent type: {message_delta.type}\")\n",
    "        \n",
    "        final_message = await stream.get_final_message()\n",
    "        print(\"\\n\\nFinal accumulated message:\", final_message.to_json())\n",
    "\n",
    "# Usage\n",
    "await stream_with_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4425373-1d34-43fb-bfe2-94853a894b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
